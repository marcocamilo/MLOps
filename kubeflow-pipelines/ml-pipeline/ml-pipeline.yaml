# PIPELINE DEFINITION
# Name: ml-pipeline
# Description: End-to-end ML pipeline that generates data, validates it, trains challenger models, and evaluates performance.
#              Pipeline flow:
#              1. Generate synthetic dataset
#              2. Validate data quality and schema
#              3. Preprocess data (train/val/test splits)
#              4. Train challenger models (currently SVC)
#              5. Evaluate all models against test set
# Inputs:
#    experiment_name: str [Default: 'KFP - Kubeflow Pipelines']
components:
  comp-condition-1:
    dag:
      tasks:
        evaluate-model:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-evaluate-model
          dependentTasks:
          - preprocess-data
          - train-svc
          - train-svc-2
          inputs:
            artifacts:
              test_set:
                taskOutputArtifact:
                  outputArtifactKey: test_set
                  producerTask: preprocess-data
            parameters:
              experiment_name:
                componentInputParameter: pipelinechannel--experiment_name
              run_id_1:
                taskOutputParameter:
                  outputParameterKey: run_id
                  producerTask: train-svc
              run_id_2:
                taskOutputParameter:
                  outputParameterKey: run_id
                  producerTask: train-svc-2
          taskInfo:
            name: evaluate-model
        preprocess-data:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-preprocess-data
          inputs:
            artifacts:
              dataset:
                componentInputArtifact: pipelinechannel--validate-data-validated_dataset
          taskInfo:
            name: preprocess-data
        train-svc:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-train-svc
          dependentTasks:
          - preprocess-data
          inputs:
            artifacts:
              train_set:
                taskOutputArtifact:
                  outputArtifactKey: train_set
                  producerTask: preprocess-data
              val_set:
                taskOutputArtifact:
                  outputArtifactKey: val_set
                  producerTask: preprocess-data
            parameters:
              experiment_name:
                componentInputParameter: pipelinechannel--experiment_name
          taskInfo:
            name: train-svc
        train-svc-2:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-train-svc-2
          dependentTasks:
          - preprocess-data
          inputs:
            artifacts:
              train_set:
                taskOutputArtifact:
                  outputArtifactKey: train_set
                  producerTask: preprocess-data
              val_set:
                taskOutputArtifact:
                  outputArtifactKey: val_set
                  producerTask: preprocess-data
            parameters:
              experiment_name:
                componentInputParameter: pipelinechannel--experiment_name
          taskInfo:
            name: train-svc-2
    inputDefinitions:
      artifacts:
        pipelinechannel--validate-data-validated_dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        pipelinechannel--experiment_name:
          parameterType: STRING
        pipelinechannel--validate-data-validated:
          parameterType: BOOLEAN
  comp-evaluate-model:
    executorLabel: exec-evaluate-model
    inputDefinitions:
      artifacts:
        test_set:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        experiment_name:
          parameterType: STRING
        run_id_1:
          parameterType: STRING
        run_id_2:
          parameterType: STRING
  comp-generate-data:
    executorLabel: exec-generate-data
    outputDefinitions:
      artifacts:
        dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-preprocess-data:
    executorLabel: exec-preprocess-data
    inputDefinitions:
      artifacts:
        dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        test_size:
          defaultValue: 0.3
          isOptional: true
          parameterType: NUMBER_DOUBLE
    outputDefinitions:
      artifacts:
        test_set:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        train_set:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        val_set:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-train-svc:
    executorLabel: exec-train-svc
    inputDefinitions:
      artifacts:
        train_set:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        val_set:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        experiment_name:
          parameterType: STRING
        n_trials:
          defaultValue: 3.0
          isOptional: true
          parameterType: NUMBER_INTEGER
    outputDefinitions:
      parameters:
        run_id:
          parameterType: STRING
  comp-train-svc-2:
    executorLabel: exec-train-svc-2
    inputDefinitions:
      artifacts:
        train_set:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        val_set:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        experiment_name:
          parameterType: STRING
        n_trials:
          defaultValue: 3.0
          isOptional: true
          parameterType: NUMBER_INTEGER
    outputDefinitions:
      parameters:
        run_id:
          parameterType: STRING
  comp-validate-data:
    executorLabel: exec-validate-data
    inputDefinitions:
      artifacts:
        dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        imbalance_threshold:
          defaultValue: 0.02
          isOptional: true
          parameterType: NUMBER_DOUBLE
    outputDefinitions:
      artifacts:
        validated_dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        validated:
          parameterType: BOOLEAN
deploymentSpec:
  executors:
    exec-evaluate-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - evaluate_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pandas' 'mlflow'\
          \ 'google-cloud-storage' 'matplotlib' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef evaluate_model(\n    experiment_name: str,\n    run_id_1: str,\n\
          \    run_id_2: str,\n    test_set: Input[Dataset],\n):\n    import logging\n\
          \n    import mlflow\n    import pandas as pd\n    from mlflow_utils import\
          \ initialize_mlflow\n\n    logging.basicConfig(level=logging.INFO)\n\n \
          \   try:\n        logging.info(\"\U0001F4E1 Connecting to MLflow Server...\"\
          )\n        initialize_mlflow(experiment_name=experiment_name)\n\n      \
          \  logging.info(\"\U0001F4E5 Loading test dataset...\")\n        test_df\
          \ = pd.read_parquet(test_set.path)\n\n        logging.info(\"\U0001F6D2\
          \ Collecting training artifacts...\")\n        run_ids = [run_id_1, run_id_2]\n\
          \n        logging.info(f\"\U0001F9EA Evaluating {len(run_ids)} model(s)...\"\
          )\n        for i, run_id in enumerate(run_ids, start=1):\n            try:\n\
          \                logging.info(f\"\U0001F4E6 Loading model #{i}\")\n    \
          \            with mlflow.start_run(run_id=run_id):\n                   \
          \ model_uri = f\"runs:/{run_id}/model\"\n                    model = mlflow.pyfunc.load_model(model_uri)\n\
          \n                    mlflow.evaluate(\n                        model=model,\n\
          \                        data=test_df,\n                        targets=\"\
          target\",\n                        model_type=\"classifier\",\n        \
          \            )\n                logging.info(f\"\U0001F44D Successfully\
          \ logged evaluation metrics for model #{i}\")\n            except Exception\
          \ as e:\n                raise RuntimeError(f\"Failed to evaluate model\
          \ #{i}: {e}\")\n\n        logging.info(\"\U0001F389 All model evaluations\
          \ completed!\")\n\n    except Exception as e:\n        logging.error(f\"\
          Evaluation failed: {e}\")\n        raise RuntimeError(f\"Model evaluation\
          \ failed: {e}\")\n\n"
        image: europe-docker.pkg.dev/rnd-data-ml/base-images/cpu-mlflow-py3.12
    exec-generate-data:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - generate_data
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pandas' 'scikit-learn'\
          \ 'pyarrow' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef generate_data(dataset: Output[Dataset]):\n    import logging\n\
          \    import pandas as pd\n    from sklearn.datasets import make_classification\n\
          \n    X, y = make_classification(\n        n_samples=1000, n_features=100,\
          \ n_informative=10, n_classes=20\n    )\n\n    df = pd.DataFrame(X, columns=[f\"\
          f{i}\" for i in range(X.shape[1])])\n    df[\"target\"] = y\n\n    df.to_parquet(dataset.path)\n\
          \    logging.info(\"\u2699\uFE0F Data successfully generated\")\n\n"
        image: europe-docker.pkg.dev/rnd-data-ml/base-images/cpu-mlflow-py3.12
    exec-preprocess-data:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - preprocess_data
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pandas' 'pyarrow'\
          \ 'mlflow' 'scikit-learn' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef preprocess_data(\n    dataset: Input[Dataset],\n    train_set:\
          \ Output[Dataset],\n    val_set: Output[Dataset],\n    test_set: Output[Dataset],\n\
          \    test_size: float = 0.3,\n):\n    import logging\n\n    import pandas\
          \ as pd\n    from sklearn.model_selection import train_test_split\n    from\
          \ sklearn.preprocessing import StandardScaler\n\n    df = pd.read_parquet(dataset.path)\n\
          \    X, y = df.drop(\"target\", axis=1), df[\"target\"]\n\n    X_scaled\
          \ = pd.DataFrame(\n        StandardScaler().fit_transform(X), columns=X.columns,\
          \ index=X.index\n    )\n\n    X_train, X_temp, y_train, y_temp = train_test_split(\n\
          \        X_scaled, y, test_size=test_size, random_state=42\n    )\n    X_val,\
          \ X_test, y_val, y_test = train_test_split(\n        X_temp, y_temp, test_size=0.5,\
          \ random_state=42\n    )\n\n    df_train = X_train.assign(target=y_train)\n\
          \    df_val = X_val.assign(target=y_val)\n    df_test = X_test.assign(target=y_test)\n\
          \n    logging.info(\"\U0001F9F1 Data successfully split and logged.\")\n\
          \n    for data, output in zip(\n        [df_train, df_val, df_test], [train_set,\
          \ val_set, test_set]\n    ):\n        data.to_parquet(output.path)\n\n \
          \   logging.info(f\"Train set shape: {df_train.shape}\")\n    logging.info(f\"\
          Validation set shape: {df_val.shape}\")\n    logging.info(f\"Test set shape:\
          \ {df_test.shape}\")\n\n"
        image: europe-docker.pkg.dev/rnd-data-ml/base-images/cpu-mlflow-py3.12
    exec-train-svc:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train_svc
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pandas' 'scikit-learn'\
          \ 'optuna' 'mlflow' 'xgboost' 'google-cloud-storage' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train_svc(\n    experiment_name: str,\n    train_set: Input[Dataset],\n\
          \    val_set: Input[Dataset],\n    n_trials: int = 3,\n) -> NamedTuple(\"\
          outputs\", [(\"run_id\", str)]):\n    import logging\n    from datetime\
          \ import datetime\n\n    import mlflow\n    import optuna\n    import pandas\
          \ as pd\n    from mlflow.models import infer_signature\n    from mlflow_utils\
          \ import get_run_id, initialize_mlflow\n    from sklearn.metrics import\
          \ (accuracy_score, f1_score, precision_score,\n                        \
          \         recall_score)\n    from sklearn.svm import LinearSVC\n\n    try:\n\
          \        logging.info(\"\U0001F4E1 Connecting to MLflow Server...\")\n \
          \       initialize_mlflow(experiment_name=experiment_name)\n\n        logging.info(\"\
          \U0001F4E5 Loading training and validation data...\")\n        df_train\
          \ = pd.read_parquet(train_set.path)\n        df_val = pd.read_parquet(val_set.path)\n\
          \n        X_train = df_train.drop(\"target\", axis=1).values\n        y_train\
          \ = df_train[\"target\"].values\n        X_val = df_val.drop(\"target\"\
          , axis=1).values\n        y_val = df_val[\"target\"].values\n\n        train_dataset\
          \ = mlflow.data.from_pandas(\n            df_train, targets=\"target\",\
          \ name=\"training_set\"\n        )\n        val_dataset = mlflow.data.from_pandas(\n\
          \            df_val, targets=\"target\", name=\"validation_set\"\n     \
          \   )\n\n        def objective(trial):\n            C = trial.suggest_float(\"\
          C\", 1e-3, 1e2, log=True)\n            max_iter = trial.suggest_int(\"max_iter\"\
          , 500, 2000)\n\n            try:\n                clf = LinearSVC(C=C, max_iter=max_iter,\
          \ random_state=24)\n                clf.fit(X_train, y_train)\n        \
          \        y_pred = clf.predict(X_val)\n                score = accuracy_score(y_val,\
          \ y_pred)\n\n                logging.info(f\"\U0001F916 Performing experiment\
          \ for trial {trial.number}\")\n                with mlflow.start_run(nested=True,\
          \ run_name=f\"trial_{trial.number}\"):\n                    mlflow.log_params(\n\
          \                        {\n                            \"C\": C,\n    \
          \                        \"max_iter\": max_iter,\n                     \
          \   }\n                    )\n                    mlflow.log_metric(\"accuracy_score\"\
          , score)\n\n                    metrics = {\n                        \"\
          accuracy_score\": accuracy_score(y_val, y_pred),\n                     \
          \   \"precision_score\": precision_score(\n                            y_val,\
          \ y_pred, average=\"weighted\"\n                        ),\n           \
          \             \"recall_score\": recall_score(y_val, y_pred, average=\"weighted\"\
          ),\n                        \"f1_score\": f1_score(y_val, y_pred, average=\"\
          weighted\"),\n                    }\n                    mlflow.log_metrics(metrics)\n\
          \n                return score\n\n            except Exception as e:\n \
          \               logging.warning(f\"Trial {trial.number} failed: {e}\")\n\
          \                return 0.0\n\n        logging.info(\"\u2699\uFE0F Setting\
          \ run name...\")\n        run_name = f\"LinearSVC-{datetime.now().strftime('%y%m%d-%H%M%S')}\"\
          \n        run_id, run_name = get_run_id(run_name, experiment_name)\n\n \
          \       logging.info(\"\U0001F680 Starting MLflow run...\")\n        with\
          \ mlflow.start_run(run_id=run_id, run_name=run_name) as run:\n         \
          \   mlflow.set_tags(\n                {\"estimator_name\": \"LinearSVC\"\
          , \"stage\": \"model_development\"}\n            )\n\n            logging.info(\n\
          \                f\"\U0001F3AF Starting hyperparameter tuning with Optuna\
          \ ({n_trials} trials)...\"\n            )\n            study = optuna.create_study(direction=\"\
          maximize\")\n            study.optimize(objective, n_trials=n_trials)\n\n\
          \            logging.info(f\"\u2705 Best parameters found: {study.best_params}\"\
          )\n            best_params = study.best_params\n            best_model =\
          \ LinearSVC(**best_params, random_state=24)\n            best_model.fit(X_train,\
          \ y_train)\n\n            y_pred = best_model.predict(X_val)\n         \
          \   acc = accuracy_score(y_val, y_pred)\n            logging.info(f\"\U0001F3C6\
          \ Final model accuracy: {acc:.4f}\")\n\n            metrics = {\n      \
          \          \"accuracy_score\": accuracy_score(y_val, y_pred),\n        \
          \        \"precision_score\": precision_score(y_val, y_pred, average=\"\
          weighted\"),\n                \"recall_score\": recall_score(y_val, y_pred,\
          \ average=\"weighted\"),\n                \"f1_score\": f1_score(y_val,\
          \ y_pred, average=\"weighted\"),\n            }\n            mlflow.log_metrics(metrics)\n\
          \n            mlflow.log_input(train_dataset, context=\"training\")\n  \
          \          mlflow.log_input(val_dataset, context=\"testing\")\n\n      \
          \      signature = infer_signature(X_val, y_pred)\n            mlflow.sklearn.log_model(\n\
          \                sk_model=best_model,\n                artifact_path=\"\
          model\",\n                signature=signature,\n                input_example=X_val[:5],\n\
          \            )\n\n            logging.info(\"\U0001F4BE Saving model artifact...\"\
          )\n            logging.info(\"\u2705 Model saved!\")\n\n            final_run_id\
          \ = run.info.run_id\n\n        outputs = NamedTuple(\"outputs\", [(\"run_id\"\
          , str)])\n        return outputs(final_run_id)\n    except Exception as\
          \ e:\n        logging.error(f\"\u274C Training failed: {e}\")\n        raise\
          \ RuntimeError(f\"Model training failed: {e}\")\n\n"
        image: europe-docker.pkg.dev/rnd-data-ml/base-images/cpu-mlflow-py3.12
    exec-train-svc-2:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train_svc
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pandas' 'scikit-learn'\
          \ 'optuna' 'mlflow' 'xgboost' 'google-cloud-storage' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train_svc(\n    experiment_name: str,\n    train_set: Input[Dataset],\n\
          \    val_set: Input[Dataset],\n    n_trials: int = 3,\n) -> NamedTuple(\"\
          outputs\", [(\"run_id\", str)]):\n    import logging\n    from datetime\
          \ import datetime\n\n    import mlflow\n    import optuna\n    import pandas\
          \ as pd\n    from mlflow.models import infer_signature\n    from mlflow_utils\
          \ import get_run_id, initialize_mlflow\n    from sklearn.metrics import\
          \ (accuracy_score, f1_score, precision_score,\n                        \
          \         recall_score)\n    from sklearn.svm import LinearSVC\n\n    try:\n\
          \        logging.info(\"\U0001F4E1 Connecting to MLflow Server...\")\n \
          \       initialize_mlflow(experiment_name=experiment_name)\n\n        logging.info(\"\
          \U0001F4E5 Loading training and validation data...\")\n        df_train\
          \ = pd.read_parquet(train_set.path)\n        df_val = pd.read_parquet(val_set.path)\n\
          \n        X_train = df_train.drop(\"target\", axis=1).values\n        y_train\
          \ = df_train[\"target\"].values\n        X_val = df_val.drop(\"target\"\
          , axis=1).values\n        y_val = df_val[\"target\"].values\n\n        train_dataset\
          \ = mlflow.data.from_pandas(\n            df_train, targets=\"target\",\
          \ name=\"training_set\"\n        )\n        val_dataset = mlflow.data.from_pandas(\n\
          \            df_val, targets=\"target\", name=\"validation_set\"\n     \
          \   )\n\n        def objective(trial):\n            C = trial.suggest_float(\"\
          C\", 1e-3, 1e2, log=True)\n            max_iter = trial.suggest_int(\"max_iter\"\
          , 500, 2000)\n\n            try:\n                clf = LinearSVC(C=C, max_iter=max_iter,\
          \ random_state=24)\n                clf.fit(X_train, y_train)\n        \
          \        y_pred = clf.predict(X_val)\n                score = accuracy_score(y_val,\
          \ y_pred)\n\n                logging.info(f\"\U0001F916 Performing experiment\
          \ for trial {trial.number}\")\n                with mlflow.start_run(nested=True,\
          \ run_name=f\"trial_{trial.number}\"):\n                    mlflow.log_params(\n\
          \                        {\n                            \"C\": C,\n    \
          \                        \"max_iter\": max_iter,\n                     \
          \   }\n                    )\n                    mlflow.log_metric(\"accuracy_score\"\
          , score)\n\n                    metrics = {\n                        \"\
          accuracy_score\": accuracy_score(y_val, y_pred),\n                     \
          \   \"precision_score\": precision_score(\n                            y_val,\
          \ y_pred, average=\"weighted\"\n                        ),\n           \
          \             \"recall_score\": recall_score(y_val, y_pred, average=\"weighted\"\
          ),\n                        \"f1_score\": f1_score(y_val, y_pred, average=\"\
          weighted\"),\n                    }\n                    mlflow.log_metrics(metrics)\n\
          \n                return score\n\n            except Exception as e:\n \
          \               logging.warning(f\"Trial {trial.number} failed: {e}\")\n\
          \                return 0.0\n\n        logging.info(\"\u2699\uFE0F Setting\
          \ run name...\")\n        run_name = f\"LinearSVC-{datetime.now().strftime('%y%m%d-%H%M%S')}\"\
          \n        run_id, run_name = get_run_id(run_name, experiment_name)\n\n \
          \       logging.info(\"\U0001F680 Starting MLflow run...\")\n        with\
          \ mlflow.start_run(run_id=run_id, run_name=run_name) as run:\n         \
          \   mlflow.set_tags(\n                {\"estimator_name\": \"LinearSVC\"\
          , \"stage\": \"model_development\"}\n            )\n\n            logging.info(\n\
          \                f\"\U0001F3AF Starting hyperparameter tuning with Optuna\
          \ ({n_trials} trials)...\"\n            )\n            study = optuna.create_study(direction=\"\
          maximize\")\n            study.optimize(objective, n_trials=n_trials)\n\n\
          \            logging.info(f\"\u2705 Best parameters found: {study.best_params}\"\
          )\n            best_params = study.best_params\n            best_model =\
          \ LinearSVC(**best_params, random_state=24)\n            best_model.fit(X_train,\
          \ y_train)\n\n            y_pred = best_model.predict(X_val)\n         \
          \   acc = accuracy_score(y_val, y_pred)\n            logging.info(f\"\U0001F3C6\
          \ Final model accuracy: {acc:.4f}\")\n\n            metrics = {\n      \
          \          \"accuracy_score\": accuracy_score(y_val, y_pred),\n        \
          \        \"precision_score\": precision_score(y_val, y_pred, average=\"\
          weighted\"),\n                \"recall_score\": recall_score(y_val, y_pred,\
          \ average=\"weighted\"),\n                \"f1_score\": f1_score(y_val,\
          \ y_pred, average=\"weighted\"),\n            }\n            mlflow.log_metrics(metrics)\n\
          \n            mlflow.log_input(train_dataset, context=\"training\")\n  \
          \          mlflow.log_input(val_dataset, context=\"testing\")\n\n      \
          \      signature = infer_signature(X_val, y_pred)\n            mlflow.sklearn.log_model(\n\
          \                sk_model=best_model,\n                artifact_path=\"\
          model\",\n                signature=signature,\n                input_example=X_val[:5],\n\
          \            )\n\n            logging.info(\"\U0001F4BE Saving model artifact...\"\
          )\n            logging.info(\"\u2705 Model saved!\")\n\n            final_run_id\
          \ = run.info.run_id\n\n        outputs = NamedTuple(\"outputs\", [(\"run_id\"\
          , str)])\n        return outputs(final_run_id)\n    except Exception as\
          \ e:\n        logging.error(f\"\u274C Training failed: {e}\")\n        raise\
          \ RuntimeError(f\"Model training failed: {e}\")\n\n"
        image: europe-docker.pkg.dev/rnd-data-ml/base-images/cpu-mlflow-py3.12
    exec-validate-data:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - validate_data
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pandas' 'pyarrow'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef validate_data(\n    dataset: Input[Dataset],\n    validated_dataset:\
          \ Output[Dataset],\n    imbalance_threshold: float = 0.02,\n) -> NamedTuple(\"\
          outputs\", [(\"validated\", bool)]):\n    import logging\n\n    import pandas\
          \ as pd\n\n    outputs = NamedTuple(\"outputs\", [(\"validated\", bool)])\n\
          \n    df = pd.read_parquet(dataset.path)\n\n    logging.info(\"Performing\
          \ completeness check...\")\n    columns_with_nans = df.columns[df.isna().any()].tolist()\n\
          \    if columns_with_nans:\n        logging.info(f\"\u26A0\uFE0F Missing\
          \ values detected in columns: {columns_with_nans}\")\n        return outputs(False)\n\
          \n    logging.info(\"Performing duplicate check...\")\n    duplicates =\
          \ df.duplicated().sum()\n    if duplicates > 0:\n        logging.info(f\"\
          \u26A0\uFE0F {duplicates} duplicates found\")\n        return outputs(False)\n\
          \n    logging.info(\"Checking severe class imbalance...\")\n    class_counts\
          \ = df.iloc[:, -1].value_counts(normalize=True)\n    min_class_freq = class_counts.min()\n\
          \    if min_class_freq < imbalance_threshold:\n        logging.info(\n \
          \           f\"\u26A0\uFE0F Severe imbalance detected. Minority class with\
          \ {min_class_freq:.4f} ratio\"\n        )\n        return outputs(False)\n\
          \n    logging.info(\"\u2705 Dataset validated!\")\n    df.to_parquet(validated_dataset.path)\n\
          \n    return outputs(True)\n\n"
        image: europe-docker.pkg.dev/rnd-data-ml/base-images/cpu-mlflow-py3.12
pipelineInfo:
  description: 'End-to-end ML pipeline that generates data, validates it, trains challenger
    models, and evaluates performance.

    Pipeline flow:

    1. Generate synthetic dataset

    2. Validate data quality and schema

    3. Preprocess data (train/val/test splits)

    4. Train challenger models (currently SVC)

    5. Evaluate all models against test set'
  name: ml-pipeline
root:
  dag:
    tasks:
      condition-1:
        componentRef:
          name: comp-condition-1
        dependentTasks:
        - validate-data
        inputs:
          artifacts:
            pipelinechannel--validate-data-validated_dataset:
              taskOutputArtifact:
                outputArtifactKey: validated_dataset
                producerTask: validate-data
          parameters:
            pipelinechannel--experiment_name:
              componentInputParameter: experiment_name
            pipelinechannel--validate-data-validated:
              taskOutputParameter:
                outputParameterKey: validated
                producerTask: validate-data
        taskInfo:
          name: check_validation
        triggerPolicy:
          condition: inputs.parameter_values['pipelinechannel--validate-data-validated']
            == true
      generate-data:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-generate-data
        taskInfo:
          name: generate-data
      validate-data:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-validate-data
        dependentTasks:
        - generate-data
        inputs:
          artifacts:
            dataset:
              taskOutputArtifact:
                outputArtifactKey: dataset
                producerTask: generate-data
        taskInfo:
          name: validate-data
  inputDefinitions:
    parameters:
      experiment_name:
        defaultValue: KFP - Kubeflow Pipelines
        description: MLflow experiment name for tracking runs and models
        isOptional: true
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.13.0
